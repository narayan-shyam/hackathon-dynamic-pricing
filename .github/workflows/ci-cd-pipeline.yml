name: Dynamic Pricing ML Pipeline CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC for scheduled retraining checks
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.9'
  AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
  AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
  AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  AZURE_RESOURCE_GROUP: ${{ secrets.AZURE_RESOURCE_GROUP }}
  AZURE_ML_WORKSPACE: ${{ secrets.AZURE_ML_WORKSPACE }}

jobs:
  # Job 1: Code Quality and Testing
  test-and-quality:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8 black isort mypy
    
    - name: Code formatting check (Black)
      run: |
        black --check --diff . || echo "Code formatting issues found"
    
    - name: Import sorting check (isort)
      run: |
        isort --check-only --diff . || echo "Import sorting issues found"
    
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
    
    - name: Type checking with mypy
      run: |
        mypy --ignore-missing-imports notebooks/ api/ ui/ || echo "Type checking issues found"
    
    - name: Run unit tests
      run: |
        python -m pytest -xvs --tb=short
    
    - name: Archive test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results
        path: |
          .pytest_cache/
          test-results.xml

  # Job 2: Data Validation and Model Testing
  data-and-model-validation:
    runs-on: ubuntu-latest
    needs: test-and-quality
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Generate sample data
      run: |
        python generate_sample_data.py
    
    - name: Validate data schema
      run: |
        python -c "
        import pandas as pd
        import sys
        import os
        
        # Load and validate data files
        required_files = [
            'data/sample/sales_data.csv',
            'data/sample/competitor_data.csv', 
            'data/sample/customer_behavior_data.csv',
            'data/sample/inventory_data.csv'
        ]
        
        for file in required_files:
            try:
                if os.path.exists(file):
                    df = pd.read_csv(file)
                    print(f'âœ“ {file}: {len(df)} rows, {len(df.columns)} columns')
                    
                    # Check for missing values
                    missing = df.isnull().sum().sum()
                    if missing > 0:
                        print(f'  Warning: {missing} missing values')
                else:
                    print(f'âœ— File not found: {file}')
                    sys.exit(1)
                
            except Exception as e:
                print(f'âœ— Error loading {file}: {e}')
                sys.exit(1)
        
        print('Data validation completed successfully')
        "
    
    - name: Test data preprocessing
      run: |
        python notebooks/01_data_preprocessing.py
    
    - name: Test model training
      run: |
        python notebooks/02_model_training.py
    
    - name: Run comprehensive tests
      run: |
        python notebooks/03_testing_framework.py
    
    - name: Validate model artifacts
      run: |
        python -c "
        import os
        import joblib
        import pandas as pd
        
        # Check if processed data exists
        processed_data_path = 'data/processed/processed_pricing_data.csv'
        if os.path.exists(processed_data_path):
            df = pd.read_csv(processed_data_path)
            print(f'âœ“ Processed data exists: {processed_data_path} ({len(df)} rows)')
        else:
            print(f'âœ— Missing processed data: {processed_data_path}')
        
        # Check if model exists (may not exist in CI, that's ok)
        model_paths = [
            'models/dynamic_pricing_model.pkl',
            'data/processed/processed_pricing_data.csv'
        ]
        
        for path in model_paths:
            if os.path.exists(path):
                print(f'âœ“ Artifact exists: {path}')
            else:
                print(f'â„¹ï¸ Artifact not found (expected in CI): {path}')
        
        print('Model artifact validation completed')
        "

  # Job 3: Security and Dependency Scanning
  security-scan:
    runs-on: ubuntu-latest
    needs: test-and-quality
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install safety and bandit
      run: |
        pip install safety bandit
    
    - name: Check dependencies for security vulnerabilities
      run: |
        safety check --json || echo "Security vulnerabilities found in dependencies"
    
    - name: Run bandit security linter
      run: |
        bandit -r . -x tests/,venv/ -f json || echo "Security issues found in code"
    
    - name: Scan for secrets
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD

  # Job 4: Build and Test Docker Images
  build-images:
    runs-on: ubuntu-latest
    needs: [test-and-quality, data-and-model-validation]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Generate sample data for Docker build
      run: |
        python -m pip install pandas numpy
        python generate_sample_data.py
        python notebooks/01_data_preprocessing.py
        python notebooks/02_model_training.py
    
    - name: Build API Docker image
      run: |
        cd api
        docker build -t dynamic-pricing-api:${{ github.sha }} .
    
    - name: Test API container
      run: |
        # Start container
        docker run -d --name api-test -p 8000:8000 dynamic-pricing-api:${{ github.sha }}
        
        # Wait for container to start
        sleep 30
        
        # Test health endpoint
        curl -f http://localhost:8000/health || echo "API health check failed"
        
        # Stop container
        docker stop api-test
        docker rm api-test
    
    - name: Build UI Docker image
      run: |
        cd ui
        docker build -t dynamic-pricing-ui:${{ github.sha }} .
    
    - name: Test UI container
      run: |
        # Start UI container
        docker run -d --name ui-test -p 8501:8501 \
          -e API_BASE_URL=http://host.docker.internal:8000 \
          dynamic-pricing-ui:${{ github.sha }}
        
        # Wait for container to start
        sleep 30
        
        # Test Streamlit health
        curl -f http://localhost:8501/_stcore/health || echo "UI health check failed"
        
        # Stop container
        docker stop ui-test
        docker rm ui-test

  # Job 5: Deploy to Staging
  deploy-staging:
    runs-on: ubuntu-latest
    needs: [security-scan, build-images]
    if: github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Deploy to Azure Container Instances (Staging)
      run: |
        # Create resource group if it doesn't exist
        az group create --name rg-dynamic-pricing-staging --location eastus || echo "Resource group exists"
        
        # Deploy API container
        az container create \
          --resource-group rg-dynamic-pricing-staging \
          --name dynamic-pricing-api-staging-${{ github.run_number }} \
          --image dynamic-pricing-api:${{ github.sha }} \
          --dns-name-label dynamic-pricing-api-staging-${{ github.run_number }} \
          --ports 8000 \
          --environment-variables \
            MLFLOW_TRACKING_URI=${{ env.MLFLOW_TRACKING_URI }} \
          --secure-environment-variables \
            AZURE_CLIENT_SECRET=${{ env.AZURE_CLIENT_SECRET }} || echo "API deployment failed"
        
        # Deploy UI container
        az container create \
          --resource-group rg-dynamic-pricing-staging \
          --name dynamic-pricing-ui-staging-${{ github.run_number }} \
          --image dynamic-pricing-ui:${{ github.sha }} \
          --dns-name-label dynamic-pricing-ui-staging-${{ github.run_number }} \
          --ports 8501 \
          --environment-variables \
            API_BASE_URL=http://dynamic-pricing-api-staging-${{ github.run_number }}.eastus.azurecontainer.io:8000 || echo "UI deployment failed"
    
    - name: Run staging tests
      run: |
        # Wait for deployment
        sleep 90
        
        # Test staging API
        curl -f http://dynamic-pricing-api-staging-${{ github.run_number }}.eastus.azurecontainer.io:8000/health || echo "Staging API test failed"
        
        # Test staging UI
        curl -f http://dynamic-pricing-ui-staging-${{ github.run_number }}.eastus.azurecontainer.io:8501/_stcore/health || echo "Staging UI test failed"

  # Job 6: ML Model Validation and Registration
  model-validation:
    runs-on: ubuntu-latest
    needs: deploy-staging
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install azure-ai-ml
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Generate and validate model
      run: |
        # Generate data and train model for validation
        python generate_sample_data.py
        python notebooks/01_data_preprocessing.py
        python notebooks/02_model_training.py
        
        python -c "
        import mlflow
        import mlflow.sklearn
        import os
        import pandas as pd
        import numpy as np
        import joblib
        from sklearn.metrics import r2_score
        
        # Set MLflow tracking URI
        mlflow.set_tracking_uri('file:./mlruns')
        
        # Load the trained model
        model_path = 'models/dynamic_pricing_model.pkl'
        if os.path.exists(model_path):
            model = joblib.load(model_path)
            print('âœ“ Model loaded successfully')
            
            # Generate test data for validation
            np.random.seed(42)
            test_data = pd.DataFrame({
                'MRP': np.random.uniform(80, 120, 100),
                'NoPromoPrice': np.random.uniform(70, 110, 100),
                'SellingPrice': np.random.uniform(60, 100, 100),
                'CTR': np.random.uniform(0.01, 0.05, 100),
                'AbandonedCartRate': np.random.uniform(0.1, 0.3, 100),
                'BounceRate': np.random.uniform(0.2, 0.5, 100),
                'IsMetro': np.random.choice([0, 1], 100),
                'month': np.random.randint(1, 13, 100),
                'day': np.random.randint(1, 29, 100),
                'dayofweek': np.random.randint(1, 8, 100),
                'quarter': np.random.randint(1, 5, 100),
                'competitor_price': np.random.uniform(65, 105, 100)
            })
            
            predictions = model.predict(test_data)
            
            # Validation criteria
            if len(predictions) == len(test_data) and all(p >= 0 for p in predictions):
                print('âœ“ Model validation passed')
                print(f'âœ“ Generated {len(predictions)} valid predictions')
                print(f'âœ“ Prediction range: {predictions.min():.2f} - {predictions.max():.2f}')
                
                # Register model with MLflow
                with mlflow.start_run():
                    mlflow.sklearn.log_model(model, 'model')
                    mlflow.log_param('validation_samples', len(test_data))
                    mlflow.log_param('github_sha', os.environ.get('GITHUB_SHA', 'unknown'))
                    mlflow.log_metric('validation_predictions_count', len(predictions))
                    print('âœ“ Model logged to MLflow')
                
            else:
                print('âœ— Model validation failed')
                exit(1)
        else:
            print('âœ— Model file not found')
            exit(1)
        "

  # Job 7: Deploy to Production
  deploy-production:
    runs-on: ubuntu-latest
    needs: model-validation
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Deploy API to Azure App Service
      run: |
        # Create App Service Plan
        az appservice plan create \
          --name asp-dynamic-pricing-prod \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --sku B1 \
          --is-linux || echo "App Service Plan exists"
        
        # Create Web App
        az webapp create \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --plan asp-dynamic-pricing-prod \
          --name dynamic-pricing-api-prod-${{ github.run_number }} \
          --deployment-container-image-name dynamic-pricing-api:${{ github.sha }} || echo "API deployment failed"
        
        # Configure App Settings
        az webapp config appsettings set \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --name dynamic-pricing-api-prod-${{ github.run_number }} \
          --settings \
            MLFLOW_TRACKING_URI=${{ env.MLFLOW_TRACKING_URI }} \
            AZURE_SUBSCRIPTION_ID=${{ env.AZURE_SUBSCRIPTION_ID }} \
            AZURE_RESOURCE_GROUP=${{ env.AZURE_RESOURCE_GROUP }} \
            WEBSITES_PORT=8000 || echo "App settings configuration failed"
    
    - name: Deploy UI to Azure Container Instances
      run: |
        az container create \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --name dynamic-pricing-ui-prod-${{ github.run_number }} \
          --image dynamic-pricing-ui:${{ github.sha }} \
          --dns-name-label dynamic-pricing-ui-prod-${{ github.run_number }} \
          --ports 8501 \
          --environment-variables \
            API_BASE_URL=https://dynamic-pricing-api-prod-${{ github.run_number }}.azurewebsites.net || echo "UI deployment failed"
    
    - name: Run production smoke tests
      run: |
        sleep 120  # Wait for deployment
        
        # Get production URLs
        API_URL=$(az webapp show \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --name dynamic-pricing-api-prod-${{ github.run_number }} \
          --query hostNames[0] -o tsv) || echo "Could not get API URL"
        
        if [ ! -z "$API_URL" ]; then
          # Test production API
          curl -f https://$API_URL/health || echo "Production API health check failed"
          curl -f https://$API_URL/model/info || echo "Production API model info failed"
          
          # Test prediction endpoint with sample data
          curl -X POST https://$API_URL/predict \
            -H "Content-Type: application/json" \
            -d '{
              "MRP": 100.0,
              "NoPromoPrice": 90.0,
              "SellingPrice": 80.0,
              "CTR": 0.025,
              "AbandonedCartRate": 0.2,
              "BounceRate": 0.3,
              "IsMetro": true,
              "month": 6,
              "day": 15,
              "dayofweek": 3,
              "quarter": 2,
              "competitor_price": 85.0
            }' || echo "Production API prediction test failed"
        fi

  # Job 8: Automated Retraining Check
  retraining-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Check retraining triggers
      run: |
        python -c "
        import requests
        import json
        import os
        from datetime import datetime, timedelta
        
        # Check if retraining is needed
        try:
            print('Checking retraining triggers...')
            
            # Simulate checking model performance
            # In production, this would check actual monitoring data
            performance_degraded = False  # Would check actual metrics
            data_drift_detected = False   # Would check drift metrics
            time_since_last_train = 7     # Days since last training
            
            # Check time-based trigger
            if time_since_last_train >= 7:
                print('Retraining triggered: Time-based trigger (7+ days)')
                exit(1)  # Exit with error to trigger issue creation
            
            if performance_degraded:
                print('Retraining triggered: Performance degradation detected')
                exit(1)
                
            if data_drift_detected:
                print('Retraining triggered: Data drift detected')
                exit(1)
                
            print('No retraining needed at this time')
                
        except Exception as e:
            print(f'Error checking retraining triggers: {e}')
            exit(1)
        "
    
    - name: Create issue if retraining needed
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'Automated Retraining Required',
            body: 'The automated retraining check has detected that model retraining may be required. Please review the monitoring data and consider triggering a retraining pipeline.',
            labels: ['ml-ops', 'retraining', 'automated']
          })

  # Job 9: Cleanup and Notifications
  cleanup-and-notify:
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: always()
    
    steps:
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
      continue-on-error: true
    
    - name: Clean up staging resources
      if: success()
      run: |
        az group delete --name rg-dynamic-pricing-staging --yes --no-wait || echo "Staging cleanup failed"
    
    - name: Notify team on success
      if: success() && github.ref == 'refs/heads/main'
      uses: actions/github-script@v6
      with:
        script: |
          const deploymentUrl = `https://dynamic-pricing-api-prod-${{ github.run_number }}.azurewebsites.net`;
          const uiUrl = `http://dynamic-pricing-ui-prod-${{ github.run_number }}.eastus.azurecontainer.io:8501`;
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `ðŸš€ Production Deployment Successful - Build ${{ github.run_number }}`,
            body: `
            ## ðŸš€ Production Deployment Completed Successfully
            
            **Build:** ${{ github.run_number }}
            **Commit:** ${{ github.sha }}
            **Branch:** ${{ github.ref }}
            
            ### Deployed Services:
            - **API:** ${deploymentUrl}
            - **UI Dashboard:** ${uiUrl}
            
            ### What was deployed:
            - âœ… ML Model Pipeline
            - âœ… FastAPI Backend
            - âœ… Streamlit Dashboard
            - âœ… Model registered in MLflow
            - âœ… Production smoke tests passed
            
            ### Next Steps:
            - Monitor application performance
            - Review automated retraining schedule
            - Check monitoring dashboards
            `,
            labels: ['deployment', 'production', 'success']
          })
    
    - name: Notify team on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `âŒ Deployment Failed - Build ${{ github.run_number }}`,
            body: `
            ## âŒ Deployment Failed
            
            **Build:** ${{ github.run_number }}
            **Commit:** ${{ github.sha }}
            **Branch:** ${{ github.ref }}
            
            Please check the workflow logs for details and fix the issues.
            
            [View Workflow Run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `,
            labels: ['deployment', 'failure', 'urgent']
          })

  # Job 10: Performance Monitoring Setup
  setup-monitoring:
    runs-on: ubuntu-latest
    needs: deploy-production
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Setup Azure Application Insights
      run: |
        # Create Application Insights
        az monitor app-insights component create \
          --app dynamic-pricing-insights-${{ github.run_number }} \
          --location eastus \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --application-type web || echo "Application Insights creation failed"
        
        # Get instrumentation key
        INSTRUMENTATION_KEY=$(az monitor app-insights component show \
          --app dynamic-pricing-insights-${{ github.run_number }} \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --query instrumentationKey -o tsv) || echo "Could not get instrumentation key"
        
        echo "Application Insights configured with key: $INSTRUMENTATION_KEY"
    
    - name: Setup monitoring alerts
      run: |
        # Create metric alerts for API performance
        az monitor metrics alert create \
          --name "API Response Time High ${{ github.run_number }}" \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --scopes /subscriptions/${{ env.AZURE_SUBSCRIPTION_ID }}/resourceGroups/${{ env.AZURE_RESOURCE_GROUP }}/providers/Microsoft.Web/sites/dynamic-pricing-api-prod-${{ github.run_number }} \
          --condition "avg responseTime > 5" \
          --description "Alert when API response time exceeds 5 seconds" || echo "Response time alert creation failed"
        
        # Create alert for error rate
        az monitor metrics alert create \
          --name "API Error Rate High ${{ github.run_number }}" \
          --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
          --scopes /subscriptions/${{ env.AZURE_SUBSCRIPTION_ID }}/resourceGroups/${{ env.AZURE_RESOURCE_GROUP }}/providers/Microsoft.Web/sites/dynamic-pricing-api-prod-${{ github.run_number }} \
          --condition "avg Http5xx > 5" \
          --description "Alert when API error rate exceeds 5 errors per minute" || echo "Error rate alert creation failed"
